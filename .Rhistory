# Changing categorical to numeric for inclusion probability
adult.native.fac <- factor(adult.cleaned$native.country)
length(adult.native.fac)
levels(adult.native.fac) <- c(1:41)
adult.native.cleaned <- adult.cleaned
adult.native.cleaned$native.country <- as.numeric(adult.native.fac)
levels(adult.native.cleaned$native.country)
pik <- inclusionprobabilities(adult.native.cleaned$native.country, n)
length(pik)
sum(pik)
s.sys.unequal <- UPsystematic(pik)
table(s.sys.unequal)
adult.sys.unequal <- adult.cleaned[s.sys.unequal!=0, ]
table(adult.sys.unequal$native.country)
# -- Strata sampling : unequal probility --
freq <- table(adult.cleaned$native.country)
st.sizes <- round(n*freq/sum(freq))
if(sum(st.sizes) > n) {
zero.pointer <- match(0, st.sizes)
max.pointer <- match(max(st.sizes), st.sizes)
if(zero.pointer != 0 | is.na(zero.pointer) != FALSE) {
st.sizes[zero.pointer] <- table(adult.cleaned$native.country)[[zero.pointer]]
st.sizes[max.pointer] <- st.sizes[max.pointer] - st.sizes[zero.pointer]
}
second.high <- match(sort(st.sizes, decreasing = TRUE)[2], st.sizes)
st.sizes[second.high] <- st.sizes[second.high] - (sum(st.sizes) - n)
}
sum(st.sizes)
length(st.sizes)
sort(table(adult.cleaned$native.country))
s.strata <- strata(adult.cleaned[order(adult.cleaned$native.country), ],
stratanames = c("native.country"), size = st.sizes, method = "srswor", description = TRUE)
adult.strata <- getdata(adult.cleaned, s.strata)
table(adult.strata$native.country)
# There are 41 different countries (levels) in the native.country attribute
# Stratified sampling is the best option in this case
unique(adult$native.country) # All conuntries taken by the population
length(unique(adult$native.country)) # the number of all countries, which is 41
nrow(table(adult.srs.w.rep$native.country))
nrow(table(adult.srs.wo.rep$native.country))
nrow(table(adult.sys$native.country))
nrow(table(adult.sys.unequal$native.country))
nrow(table(adult.strata$native.country)) # only stratified sampling does a good job picking up every 41 country sample as an effective sampling
# Confidence Intervals
population.mean <- mean(adult$fnlwgt)
population.sd <- sd(adult$fnlwgt)
sd.sample.means <- population.sd/sqrt(5)
sd(final.weight.bar.5)
sd.sample.means
sd.sample.means <- population.sd/sqrt(100)
sd(final.weight.bar.100)
sd.sample.means
sd.sample.means <- population.sd/sqrt(500)
sd(final.weight.bar.500)
sd.sample.means
population.mean
final.weight.mean
final.weight.sd
samples.size
conf80.lower <- numeric(length(samples.size))
conf80.lower
conf80.upper <- numeric(length(samples.size))
conf80.upper
samples.size
conf90.lower <- numeric(length(samples.size))
conf90.upper <- numeric(length(samples.size))
for (i in 1:length(samples.size)){
conf80.lower[i] <- final.weight.mean[i] - 1.28*(population.sd/sqrt(samples.size[i]))
conf80.upper[i] <- final.weight.mean[i] + 1.28*(population.sd/sqrt(samples.size[i]))
conf90.lower[i] <- final.weight.mean[i] - 1.645*(population.sd/sqrt(samples.size[i]))
conf90.upper[i] <- final.weight.mean[i] + 1.645*(population.sd/sqrt(samples.size[i]))
}
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="blue")
plot(samples.size, conf90.lower, type="l", col="deeppink2", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="deeppink2")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="deeppink2", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="deeppink2")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="blue")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="darkgreen2")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="deeppink2", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="deeppink2")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="darkgreen")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="deeppink2", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="deeppink2")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="green")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="grey30", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="grey30")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="red")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="grey20", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="grey20")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="red")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="blue", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="blue")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="red")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", ylim = c(0,max(conf90.upper)),
xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey")
?rev
abline(mod)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col="lightgrey",border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col="grey15",border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col="grey79",border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col="grey90",border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.5),border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.2),border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.9),border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.05),border=NA)
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.05),border=NA)
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="black")
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey15")
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey15")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey35")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey35")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
ar(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
plot(samples.size, conf80.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf80.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf80.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf80.lower,rev(conf80.upper)),col=rgb(1, 0, 0,0.1),border=NA)
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
plot(samples.size, conf80.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf80.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 80%")
lines(samples.size, conf80.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf80.lower,rev(conf80.upper)),col=rgb(1, 0, 0,0.1),border=NA)
conf80.lower <- numeric(length(samples.size))
conf80.upper <- numeric(length(samples.size))
conf90.lower <- numeric(length(samples.size))
conf90.upper <- numeric(length(samples.size))
for (i in 1:length(samples.size)){
conf80.lower[i] <- final.weight.mean[i] - 1.28*(population.sd/sqrt(samples.size[i]))
conf80.upper[i] <- final.weight.mean[i] + 1.28*(population.sd/sqrt(samples.size[i]))
conf90.lower[i] <- final.weight.mean[i] - 1.645*(population.sd/sqrt(samples.size[i]))
conf90.upper[i] <- final.weight.mean[i] + 1.645*(population.sd/sqrt(samples.size[i]))
}
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
plot(samples.size, conf80.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf80.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 80%")
lines(samples.size, conf80.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf80.lower,rev(conf80.upper)),col=rgb(1, 0, 0,0.1),border=NA)
par(mfrow=c(1,1))
xbar <- numeric(20)
xbar <- numeric(20)
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:20){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("80% confidence intervals =",
xbar[j] - 1.28*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.28*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:20){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("90% confidence intervals =",
xbar[j] - 1.645*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.645*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
samples.size
xbar <- numeric(20)
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:20){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("80% confidence intervals =",
xbar[j] - 1.28*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.28*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:20){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("90% confidence intervals =",
xbar[j] - 1.645*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.645*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
xbar <- numeric(3)
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:3){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("80% confidence intervals =",
xbar[j] - 1.28*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.28*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:3){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("90% confidence intervals =",
xbar[j] - 1.645*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.645*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
xbar <- numeric(20)
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:20){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("80% confidence intervals =",
xbar[j] - 1.28*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.28*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
for (i in 1:length(samples.size)){
cat("Sample.size : ",samples.size[i],"\n")
for (j in 1:20){
xbar[j] <- mean(sample(final.weight,samples.size[i]))
cat("90% confidence intervals =",
xbar[j] - 1.645*(population.sd/sqrt(samples.size[i]))," - ",
xbar[j] + 1.645*(population.sd/sqrt(samples.size[i])),  "\n")
}
}
population.mean
par(mfrow=c(1,2))
plot(samples.size, conf90.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf90.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 90%")
lines(samples.size, conf90.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf90.lower,rev(conf90.upper)),col=rgb(1, 0, 0,0.1),border=NA)
plot(samples.size, conf80.lower, type="l", col="red", lty = 'dashed',
ylim = c(0,max(conf80.upper)), xlab="Sample size", ylab="values", main="Confidence Interval 80%")
lines(samples.size, conf80.upper, type="l", col="red", lty = 'dashed')
lines(samples.size, rep(population.mean, length(samples.size)), type="l", col="grey65")
polygon(c(samples.size,rev(samples.size)), c(conf80.lower,rev(conf80.upper)),col=rgb(1, 0, 0,0.1),border=NA)
par(mfrow=c(1,1))
# -- Importing dataset into R --
adult <- read.csv("adult.csv")
dim(adult)
names(adult)
head(adult)
nrow(higherPaid)
nrow(lowerPaid)
head(adult, n=1)
tail(adult, n=1)
adult <- read.csv("adult.csv")
dim(adult)
names(adult)
head(adult)
# -- Preprocessing --
# Splitting two classes depending on the income earned
higherPaid <-adult[adult$income == ">50K", ]
lowerPaid <- adult[adult$income == "<=50K", ]
nrow(higherPaid)
nrow(lowerPaid)
# Finding and imputing the unknown values with mean or median
length(table(adult$workclass))
length(table(adult$occupation))
length(sort(table(adult$native.country)))
length(table(adult$relationship))
class(higherPaid)
str(adult)
glm(adult$income~., family = binomial())
glm(adult$income~., family = binomial(), adult)
head(data, n=1)
head(adult, n=1)
caret::confusionMatrix(predicted.class, test$income, positive = ">50k")
predicted.class <- ifelse(predicted > 0.5, ">50k", "<=50k")
predicted.class <- ifelse(predicted > 0.5, ">50k", "<=50k")
adult <- read.csv('adult.csv')
str(adult)
table(adult$workclass)
# simplify workclass categories by combining similar ones
table(adult$workclass)
adult$workclass <- as.character(adult$workclass)
adult$workclass[adult$workclass == "Without-pay" | adult$workclass == "Never-worked"] <- "Unemployed"
adult$workclass[adult$workclass == "State-gov" | adult$workclass == "Local-gov"] <- "SL-gov"
adult$workclass[adult$workclass == "Self-emp-inc" | adult$workclass == "Self-emp-not-inc"] <- "Self-employed"
table(adult$workclass)
# simplify marital.status categories by combining into 3 types (Married, Not-Married, Never-Married)
table(adult$marital.status)
adult$marital.status <- as.character(adult$marital.status)
adult$marital.status[adult$marital.status == "Married-AF-spouse"
| adult$marital.status == "Married-civ-spouse"
| adult$marital.status == "Married-spouse-absent"] <- "Married"
adult$marital.status[adult$marital.status == "Divorced"
| adult$marital.status == "Separated"
| adult$marital.status == "Widowed"] <- "Not-Married"
table(adult$marital.status)
# simplify countries by reduce them to their respective regions
table(adult$native.country)
adult$native.country <- as.character(adult$native.country)
north.america <- c("Canada", "Cuba", "Dominican-Republic", "El-Salvador", "Guatemala",
"Haiti", "Honduras", "Jamaica", "Mexico", "Nicaragua",
"Outlying-US(Guam-USVI-etc)", "Puerto-Rico", "Trinadad&Tobago",
"United-States")
asia <- c("Cambodia", "China", "Hong", "India", "Iran", "Japan", "Laos",
"Philippines", "Taiwan", "Thailand", "Vietnam")
south.america <- c("Columbia", "Ecuador", "Peru")
europe <- c("England", "France", "Germany", "Greece", "Holand-Netherlands",
"Hungary", "Ireland", "Italy", "Poland", "Portugal", "Scotland",
"Yugoslavia")
other <- c("South", "?")
adult$native.country[adult$native.country %in% north.america] <- "North America"
adult$native.country[adult$native.country %in% asia] <- "Asia"
adult$native.country[adult$native.country %in% south.america] <- "South America"
adult$native.country[adult$native.country %in% europe] <- "Europe"
adult$native.country[adult$native.country %in% other] <- "Other"
table(adult$native.country)
# set all variables back to factors from characters so we could edit them
adult$workclass <- as.factor(adult$workclass)
adult$marital.status <- as.factor(adult$marital.status)
adult$native.country <- as.factor(adult$native.country)
str(adult)
# deal with missing value
table(adult$workclass)
adult[adult == "?"] <- NA  # convert "?" to NA and discard them to make the model efficient
table(adult$workclass)
#install.packages("Amelia")
# Amelia package is used to map missing value in the dataset
library(Amelia)
missmap(adult, y.at = 1, y.labels = "", col = c("red", "black"), legend = FALSE)
# omit the value since most NA values are present in the occupation and workclass
adult <- na.omit(adult)
# EDA
#install.packages("ggplot2")
library(ggplot2)
ggplot(adult, aes(age)) + geom_histogram(aes(fill = income), color = "black", binwidth = 1)
# in the above histogram people around the age between 25-50 tend to ear more than 50k
ggplot(adult, aes(hours.per.week)) + geom_histogram()
# in the above historgram the highest frequency of hours.per.week occurs at 40
library(data.table)
setnames(adult, "native.country", "region")
region.ordered <- reorder(adult$region, adult$region, length)
region.ordered <- factor(region.ordered, levels = rev(levels(region.ordered)))
ggplot(adult, aes(region.ordered)) + geom_bar(aes(fill = income), color = "black")
library(caTools)
library(caret)
set.seed(123)
split <- sample.split(adult$income, SplitRatio = 0.7)
train <- subset(adult, split == TRUE)
test <- subset(adult, split = FALSE)
log.model <- glm(income ~ ., family = binomial(), train) # set the logistic regression model
summary(log.model)
predicted <- predict(log.model, test, type = "response")
predicted.class <- ifelse(predicted > 0.5, ">50k", "<=50k")
proportions <- addmargins(table(test$income, predicted.class))
caret::confusionMatrix(predicted.class, test$income, positive = ">50k")
confusionMatrix(factor(predicted.class), test$income)
proportion
proportions
str(test$income)
str(predicted.class)
length(predicted.class)
# MET CS555 Mini Assignment
# Student: Tzupin Kuo
# 05/01/2019
# Research topic: Predict whether income exceeds $50K/yr based on census data
# Source: Adult Census Income (dataset compiled by UCI machine learning repository)
# getting the data
adult <- read.csv('adult.csv')
str(adult)
table(adult$workclass)
# simplify workclass categories by combining similar ones
table(adult$workclass)
adult$workclass <- as.character(adult$workclass)
adult$workclass[adult$workclass == "Without-pay" | adult$workclass == "Never-worked"] <- "Unemployed"
adult$workclass[adult$workclass == "State-gov" | adult$workclass == "Local-gov"] <- "SL-gov"
adult$workclass[adult$workclass == "Self-emp-inc" | adult$workclass == "Self-emp-not-inc"] <- "Self-employed"
table(adult$workclass)
# simplify marital.status categories by combining into 3 types (Married, Not-Married, Never-Married)
table(adult$marital.status)
adult$marital.status <- as.character(adult$marital.status)
adult$marital.status[adult$marital.status == "Married-AF-spouse"
| adult$marital.status == "Married-civ-spouse"
| adult$marital.status == "Married-spouse-absent"] <- "Married"
adult$marital.status[adult$marital.status == "Divorced"
| adult$marital.status == "Separated"
| adult$marital.status == "Widowed"] <- "Not-Married"
table(adult$marital.status)
# simplify countries by reduce them to their respective regions
table(adult$native.country)
adult$native.country <- as.character(adult$native.country)
north.america <- c("Canada", "Cuba", "Dominican-Republic", "El-Salvador", "Guatemala",
"Haiti", "Honduras", "Jamaica", "Mexico", "Nicaragua",
"Outlying-US(Guam-USVI-etc)", "Puerto-Rico", "Trinadad&Tobago",
"United-States")
asia <- c("Cambodia", "China", "Hong", "India", "Iran", "Japan", "Laos",
"Philippines", "Taiwan", "Thailand", "Vietnam")
south.america <- c("Columbia", "Ecuador", "Peru")
europe <- c("England", "France", "Germany", "Greece", "Holand-Netherlands",
"Hungary", "Ireland", "Italy", "Poland", "Portugal", "Scotland",
"Yugoslavia")
other <- c("South", "?")
adult$native.country[adult$native.country %in% north.america] <- "North America"
adult$native.country[adult$native.country %in% asia] <- "Asia"
adult$native.country[adult$native.country %in% south.america] <- "South America"
adult$native.country[adult$native.country %in% europe] <- "Europe"
adult$native.country[adult$native.country %in% other] <- "Other"
table(adult$native.country)
# set all variables back to factors from characters so we could edit them
adult$workclass <- as.factor(adult$workclass)
adult$marital.status <- as.factor(adult$marital.status)
adult$native.country <- as.factor(adult$native.country)
str(adult)
# deal with missing value
table(adult$workclass)
adult[adult == "?"] <- NA  # convert "?" to NA and discard them to make the model efficient
table(adult$workclass)
#install.packages("Amelia")
# Amelia package is used to map missing value in the dataset
library(Amelia)
missmap(adult, y.at = 1, y.labels = "", col = c("red", "black"), legend = FALSE)
# omit the value since most NA values are present in the occupation and workclass
adult <- na.omit(adult)
# EDA
#install.packages("ggplot2")
library(ggplot2)
ggplot(adult, aes(age)) + geom_histogram(aes(fill = income), color = "black", binwidth = 1)
# in the above histogram people around the age between 25-50 tend to ear more than 50k
ggplot(adult, aes(hours.per.week)) + geom_histogram()
# in the above historgram the highest frequency of hours.per.week occurs at 40
#install.packages("data.table")
library(data.table)
setnames(adult, "native.country", "region")
region.ordered <- reorder(adult$region, adult$region, length)
region.ordered <- factor(region.ordered, levels = rev(levels(region.ordered)))
ggplot(adult, aes(region.ordered)) + geom_bar(aes(fill = income), color = "black")
# in the above bar chart we can see most of the data are from North America
# Building the Model
# Train-Test Split
#install.packages("caTools")
library(caTools)
library(caret)
set.seed(123)
split <- sample.split(adult$income, SplitRatio = 0.7)
train <- subset(adult, split == TRUE)
test <- subset(adult, split = FALSE)
#train.index <- createDataPartition(adult$income, p = .8, list = FALSE)
#train <- adult[train.index, ]
#test <- adult[-train.index, ]
# Train the model
log.model <- glm(income ~ ., family = binomial(), train) # set the logistic regression model
summary(log.model)
predicted <- predict(log.model, test, type = "response")
predicted.class <- ifelse(predicted > 0.5, ">50k", "<=50k")
proportions <- addmargins(table(test$income, predicted.class))
# or
# table(test$income, predicted > 0.5)
